# Test Suite Reference - LLM-Local-Lab

**√öltima actualizaci√≥n:** 2025-10-02
**Metodolog√≠a:** RPVEA-A (Agent-Augmented Testing-First)
**Generado por:** @test-architect agent

---

## üéØ Resumen Ejecutivo

Este documento centraliza toda la informaci√≥n sobre los tests del proyecto, facilitando:
- ‚úÖ R√°pida consulta de qu√© tests existen
- ‚úÖ C√≥mo ejecutarlos sin investigar cada vez
- ‚úÖ Qu√© valida cada test suite
- ‚úÖ Cu√°ndo usar cada tipo de test

**Total de Test Suites:** 3 (PRE, POST, Integration)
**Total de Test Functions:** 45
**Cobertura:** Embedding benchmarks (primera implementaci√≥n)

---

## üìÅ Estructura de Testing

```
tests/
‚îú‚îÄ‚îÄ pre/                              # PRE-tests (baseline validation)
‚îÇ   ‚îî‚îÄ‚îÄ test_embedding_prereqs_20251002.py
‚îú‚îÄ‚îÄ post/                             # POST-tests (acceptance criteria)
‚îÇ   ‚îî‚îÄ‚îÄ test_embedding_success_20251002.py
‚îú‚îÄ‚îÄ integration/                      # Integration tests (pipeline validation)
‚îÇ   ‚îî‚îÄ‚îÄ test_embedding_pipeline_20251002.py
‚îî‚îÄ‚îÄ __init__.py                       # Test discovery
```

---

## üî¥ PRE-TESTS: Baseline Validation

### Archivo: `tests/pre/test_embedding_prereqs_20251002.py`

**Prop√≥sito:** Validar que el sistema est√° listo ANTES de ejecutar benchmarks

**Cu√°ndo ejecutar:**
- ‚úÖ SIEMPRE antes de ejecutar embedding_benchmark.py
- ‚úÖ Parte de VALIDATE phase en RPVEA-A
- ‚úÖ Despu√©s de cambios en environment (actualizaciones de libs, etc.)

**Ejecuci√≥n:**
```bash
# Opci√≥n 1: Ejecutar como script
python tests/pre/test_embedding_prereqs_20251002.py

# Opci√≥n 2: Con pytest (m√°s detallado)
pytest tests/pre/test_embedding_prereqs_20251002.py -v

# Opci√≥n 3: Solo tests cr√≠ticos (r√°pido)
pytest tests/pre/test_embedding_prereqs_20251002.py -k "cuda or gpu" -v
```

**Exit Codes:**
- `0` = ‚úÖ Todos los tests pasaron - SAFE TO PROCEED
- `>0` = ‚ùå Al menos un test fall√≥ - DO NOT EXECUTE

---

### Tests Incluidos (17 total)

#### **Categor√≠a 1: Python Environment (2 tests)**

##### `test_python_version()`
```python
def test_python_version():
    """Python 3.10+ required for modern typing and async features"""
```
- **Valida:** Python >= 3.10
- **Cr√≠tico:** ‚ö†Ô∏è Medium (features modernas)
- **Si falla:** Actualizar Python

##### `test_required_packages_installed()`
```python
def test_required_packages_installed():
    """Core dependencies must be importable"""
```
- **Valida:** torch, sentence_transformers, numpy, transformers
- **Cr√≠tico:** üî¥ HIGH (bloqueante)
- **Si falla:** `pip install -r requirements.txt`

---

#### **Categor√≠a 2: GPU & CUDA (5 tests)**

##### `test_cuda_available()`
```python
def test_cuda_available():
    """CUDA must be available for GPU acceleration"""
```
- **Valida:** `torch.cuda.is_available() == True`
- **Cr√≠tico:** üî¥ HIGH (GPU required)
- **Si falla:** Verificar drivers NVIDIA, reinstalar PyTorch con CUDA

##### `test_gpu_count()`
```python
def test_gpu_count():
    """At least one GPU required"""
```
- **Valida:** `torch.cuda.device_count() >= 1`
- **Cr√≠tico:** üî¥ HIGH
- **Si falla:** Verificar GPUs detectadas con `nvidia-smi`

##### `test_gpu_memory_sufficient()`
```python
def test_gpu_memory_sufficient():
    """GPU must have at least 8GB VRAM"""
```
- **Valida:** GPU 0 tiene >= 8GB VRAM
- **Cr√≠tico:** ‚ö†Ô∏è Medium (embedding models necesitan ~2-4GB)
- **Si falla:** Liberar VRAM o usar GPU m√°s grande

##### `test_gpu_idle()`
```python
def test_gpu_idle():
    """GPU should not be under heavy load"""
```
- **Valida:** Utilizaci√≥n < 50%
- **Cr√≠tico:** üü° LOW (recomendado para benchmarks precisos)
- **Si falla:** Cerrar procesos que usan GPU

##### `test_gpu_compute_capability()`
```python
def test_gpu_compute_capability():
    """Compute capability 7.0+ recommended"""
```
- **Valida:** Compute capability >= 7.0 (Volta+)
- **Cr√≠tico:** üü° LOW (RTX 5090 tiene 12.0)
- **Si falla:** GPU muy antigua (no bloqueante)

---

#### **Categor√≠a 3: Model Availability (3 tests)**

##### `test_sentence_transformers_installed()`
```python
def test_sentence_transformers_installed():
    """sentence-transformers package required"""
```
- **Valida:** Puede importar `SentenceTransformer`
- **Cr√≠tico:** üî¥ HIGH
- **Si falla:** `pip install sentence-transformers`

##### `test_model_multilingual_exists()` ‚ö†Ô∏è **CONOCIDO: Puede Fallar**
```python
def test_model_multilingual_exists():
    """Validate paraphrase-multilingual model accessible"""
```
- **Valida:** Modelo en Hugging Face Hub (API check)
- **Cr√≠tico:** ‚ö†Ô∏è Medium (PERO: modelo en cach√© local funciona)
- **Si falla:**
  - Opci√≥n A: Ignorar si modelo est√° en cach√© local
  - Opci√≥n B: Autenticar con `huggingface-cli login`
- **Cach√© local:** `C:\Users\Gamer\.cache\huggingface\hub\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2`

##### `test_model_bge_legal_exists()` ‚ö†Ô∏è **SKIPPED si no hay HF_TOKEN**
```python
@pytest.mark.skipif(not os.getenv("HF_TOKEN"), reason="HF_TOKEN not set")
def test_model_bge_legal_exists():
    """Validate BGE legal model accessible"""
```
- **Valida:** Modelo BGE legal en HF Hub
- **Cr√≠tico:** üü° LOW (opcional, modelo GPU)
- **Si falla:** Modelo temporal puede no existir
- **Cach√© local:** `C:\Users\Gamer\.cache\huggingface\hub\models--dariolopez--bge-m3-es-legal-tmp-6`

---

#### **Categor√≠a 4: Dependencies (4 tests)**

##### `test_torch_version()`
```python
def test_torch_version():
    """PyTorch 2.0+ required"""
```
- **Valida:** PyTorch >= 2.0
- **Cr√≠tico:** ‚ö†Ô∏è Medium
- **Estado actual:** ‚úÖ 2.9.0.dev (OK)

##### `test_numpy_available()`
```python
def test_numpy_available():
    """NumPy required for array operations"""
```
- **Valida:** NumPy importable
- **Cr√≠tico:** üî¥ HIGH

##### `test_transformers_available()`
```python
def test_transformers_available():
    """transformers library for model backends"""
```
- **Valida:** `transformers` importable
- **Cr√≠tico:** üî¥ HIGH

##### `test_accelerate_available()`
```python
def test_accelerate_available():
    """accelerate library for GPU optimizations"""
```
- **Valida:** `accelerate` importable
- **Cr√≠tico:** ‚ö†Ô∏è Medium (ayuda con multi-GPU)

---

#### **Categor√≠a 5: File System (3 tests)**

##### `test_benchmark_script_exists()`
```python
def test_benchmark_script_exists():
    """Embedding benchmark script must exist"""
```
- **Valida:** `benchmarks/scripts/embedding_benchmark.py` existe
- **Cr√≠tico:** üî¥ HIGH
- **Ruta:** `C:\Users\Gamer\Dev\LLM-Local-Lab\benchmarks\scripts\embedding_benchmark.py`

##### `test_results_directory_writable()`
```python
def test_results_directory_writable():
    """Results directory must be writable"""
```
- **Valida:** Puede escribir en `benchmarks/results/raw/`
- **Cr√≠tico:** ‚ö†Ô∏è Medium (necesario para guardar outputs)

##### `test_disk_space_sufficient()`
```python
def test_disk_space_sufficient():
    """At least 10GB free space required"""
```
- **Valida:** >= 10GB libres en disco
- **Cr√≠tico:** üü° LOW (3.63 TB disponibles)

---

### Resultados del √öltimo Run (2025-10-02)

```
‚úÖ PASSED: 15/17 tests
‚ùå FAILED: 1 test (test_model_multilingual_exists - API auth)
‚è© SKIPPED: 1 test (test_model_bge_legal_exists - no HF_TOKEN)
```

**An√°lisis:**
- ‚úÖ Environment completamente funcional
- ‚úÖ GPUs idle y listas
- ‚úÖ Modelos en cach√© local (API check fall√≥ pero cach√© OK)
- ‚ö†Ô∏è HF API requiere auth (no bloqueante si modelos en cach√©)

---

## üü¢ POST-TESTS: Acceptance Criteria

### Archivo: `tests/post/test_embedding_success_20251002.py`

**Prop√≥sito:** Validar que el benchmark ejecut√≥ correctamente y cumple criterios de aceptaci√≥n

**Cu√°ndo ejecutar:**
- ‚úÖ DESPU√âS de ejecutar embedding_benchmark.py
- ‚úÖ Parte de ASSESS phase en RPVEA-A
- ‚úÖ Para validar optimizaciones (cambios en config, c√≥digo)

**Ejecuci√≥n:**
```bash
# Opci√≥n 1: Todos los tests
python tests/post/test_embedding_success_20251002.py

# Opci√≥n 2: Con pytest (detallado)
pytest tests/post/test_embedding_success_20251002.py -v -s

# Opci√≥n 3: Solo tests de performance
pytest tests/post/test_embedding_success_20251002.py -k "speedup or throughput" -v
```

---

### Tests Incluidos (12 total)

#### **Categor√≠a 1: Execution Validation (2 tests)**

##### `test_benchmark_runs_without_crash()` ‚ö†Ô∏è **EJECUTA EL BENCHMARK**
```python
def test_benchmark_runs_without_crash():
    """Benchmark must complete without errors"""
    # ‚ö†Ô∏è IMPORTANTE: Este test EJECUTA el benchmark!
```
- **Acci√≥n:** Ejecuta `embedding_benchmark.py` y captura output
- **Valida:** Exit code == 0 (sin crashes)
- **Duraci√≥n:** 15-20 minutos (incluye descarga de modelos si no existen)
- **Output:** Guardado en variable global `BENCHMARK_OUTPUT`

##### `test_benchmark_output_not_empty()`
```python
def test_benchmark_output_not_empty():
    """Output must contain results"""
```
- **Depende de:** `test_benchmark_runs_without_crash()`
- **Valida:** Output tiene contenido (no vac√≠o)

---

#### **Categor√≠a 2: Performance Metrics (4 tests)**

##### `test_gpu_speedup_significant()`
```python
def test_gpu_speedup_significant():
    """GPU must show >2x speedup vs CPU"""
```
- **Valida:** Speedup >= 2.0x
- **Esperado:** 6-10x en RTX 5090
- **Cr√≠tico:** üî¥ HIGH (principal objetivo)

##### `test_cpu_throughput_reasonable()`
```python
def test_cpu_throughput_reasonable():
    """CPU baseline 5-200 docs/sec"""
```
- **Valida:** 5 <= throughput_cpu <= 200
- **Esperado:** 30-50 docs/sec
- **Cr√≠tico:** ‚ö†Ô∏è Medium (baseline reference)

##### `test_gpu_throughput_high()`
```python
def test_gpu_throughput_high():
    """GPU throughput >50 docs/sec"""
```
- **Valida:** throughput_gpu > 50
- **Esperado:** 200-400 docs/sec (RTX 5090)
- **Cr√≠tico:** üî¥ HIGH

##### `test_batch_processing_scales()`
```python
def test_batch_processing_scales():
    """Large batch should show >200 docs/sec"""
```
- **Valida:** large_batch_throughput > 200
- **Esperado:** 2000-3000 docs/sec
- **Cr√≠tico:** ‚ö†Ô∏è Medium (optimizaci√≥n)

---

#### **Categor√≠a 3: Output Quality (3 tests)**

##### `test_embeddings_shape_correct()`
```python
def test_embeddings_shape_correct():
    """Embeddings must have correct shape"""
```
- **Valida:** Shape mencionado en output (100, X) donde X = 384 o 1024
- **Cr√≠tico:** üî¥ HIGH (correctitud)

##### `test_both_models_executed()`
```python
def test_both_models_executed():
    """Both CPU and GPU tests must run"""
```
- **Valida:** Output contiene "TEST 1" (CPU) y "TEST 2" (GPU)
- **Cr√≠tico:** ‚ö†Ô∏è Medium

##### `test_success_message_present()`
```python
def test_success_message_present():
    """Success indicator must be present"""
```
- **Valida:** Output contiene "[EXITO]" o "[OK]"
- **Cr√≠tico:** üü° LOW (indicador cualitativo)

---

#### **Categor√≠a 4: Resource Usage (3 tests)**

##### `test_gpu_memory_usage_reasonable()`
```python
def test_gpu_memory_usage_reasonable():
    """GPU memory usage < 16GB"""
```
- **Valida:** VRAM usado < 16GB
- **Esperado:** 2-6GB
- **Cr√≠tico:** ‚ö†Ô∏è Medium

##### `test_no_errors_in_output()`
```python
def test_no_errors_in_output():
    """No error messages in output"""
```
- **Valida:** No contiene "[ERROR]" o "Traceback"
- **Cr√≠tico:** üî¥ HIGH

##### `test_completion_message()`
```python
def test_completion_message():
    """Completion message must be present"""
```
- **Valida:** Output contiene "COMPLETADO"
- **Cr√≠tico:** üü° LOW

---

### Variables Globales Compartidas

```python
# Estos datos se comparten entre todos los tests
BENCHMARK_OUTPUT = None        # String completo del output
BENCHMARK_DURATION = None      # Segundos de ejecuci√≥n
BENCHMARK_EXIT_CODE = None     # 0 = success, >0 = error
```

**Uso:**
```python
# En tus tests puedes acceder:
def test_custom():
    assert BENCHMARK_OUTPUT is not None
    if "GPU" in BENCHMARK_OUTPUT:
        # Parse specific metrics
        pass
```

---

## üîó INTEGRATION TESTS: Pipeline Validation

### Archivo: `tests/integration/test_embedding_pipeline_20251002.py`

**Prop√≥sito:** Validar que los componentes del pipeline de embeddings funcionan correctamente juntos

**Cu√°ndo ejecutar:**
- ‚úÖ Despu√©s de cambios en c√≥digo de embeddings
- ‚úÖ Para validar edge cases y robustez
- ‚úÖ Parte de ASSESS phase (opcional, profundo)

**Ejecuci√≥n:**
```bash
# Opci√≥n 1: Todos los tests (LENTO - descarga modelos)
pytest tests/integration/test_embedding_pipeline_20251002.py -v

# Opci√≥n 2: Solo tests r√°pidos (sin descarga)
pytest tests/integration/test_embedding_pipeline_20251002.py -k "not load" -v

# Opci√≥n 3: Solo edge cases
pytest tests/integration/test_embedding_pipeline_20251002.py -k "edge or error" -v
```

---

### Tests Incluidos (16 total)

#### **Categor√≠a 1: Model Loading (2 tests)**

##### `test_sentence_transformer_loads()`
```python
@pytest.fixture(scope="module")
def test_sentence_transformer_loads():
    """Load multilingual model successfully"""
```
- **Tipo:** Fixture (compartido por otros tests)
- **Acci√≥n:** Carga `paraphrase-multilingual-MiniLM-L12-v2`
- **Duraci√≥n:** ~5-10 seg (primera vez desde cach√©)

##### `test_sentence_transformer_device_placement()`
```python
def test_sentence_transformer_device_placement(model):
    """Model can be moved to GPU"""
```
- **Valida:** `model.to('cuda:0')` funciona
- **Cr√≠tico:** üî¥ HIGH (GPU acceleration)

---

#### **Categor√≠a 2: Encoding Functionality (4 tests)**

##### `test_model_encode_single_text(model)`
```python
def test_model_encode_single_text(model):
    """Encode single document"""
```
- **Valida:** Encode de 1 texto funciona
- **Output:** Shape (384,)

##### `test_model_encode_multiple_texts(model)`
```python
def test_model_encode_multiple_texts(model):
    """Encode batch of documents"""
```
- **Valida:** Encode de lista funciona
- **Output:** Shape (N, 384)

##### `test_model_encode_empty_list(model)`
```python
def test_model_encode_empty_list(model):
    """Handle empty input gracefully"""
```
- **Valida:** No crash con lista vac√≠a
- **Edge case:** []

##### `test_model_encode_special_characters(model)`
```python
def test_model_encode_special_characters(model):
    """Handle Unicode and accents"""
```
- **Valida:** Textos con √±, √°, emojis
- **Edge case:** "A√±o espa√±ol üá™üá∏"

---

#### **Categor√≠a 3: Output Quality (4 tests)**

##### `test_embeddings_are_normalized(model)`
```python
def test_embeddings_are_normalized(model):
    """Embeddings should be L2 normalized"""
```
- **Valida:** `np.linalg.norm(embedding) ‚âà 1.0`
- **Importante:** Para cosine similarity

##### `test_embeddings_dimension_consistent(model)`
```python
def test_embeddings_dimension_consistent(model):
    """All embeddings same dimension"""
```
- **Valida:** Todos los outputs tienen shape[1] == 384

##### `test_embeddings_reproducible(model)`
```python
def test_embeddings_reproducible(model):
    """Same input ‚Üí same output"""
```
- **Valida:** Encode 2 veces da mismo resultado
- **Seed:** `torch.manual_seed(42)`

##### `test_embeddings_capture_similarity(model)`
```python
def test_embeddings_capture_similarity(model):
    """Similar texts have high cosine similarity"""
```
- **Valida:** Similarity > 0.7 para textos similares
- **Ejemplo:** "perro" vs "can" > 0.7

---

#### **Categor√≠a 4: Batch Processing (3 tests)**

##### `test_batch_size_parameter(model)`
```python
def test_batch_size_parameter(model):
    """Batch size parameter works"""
```
- **Valida:** `encode(texts, batch_size=16)` funciona

##### `test_batch_vs_sequential_consistency(model)`
```python
def test_batch_vs_sequential_consistency(model):
    """Batch and sequential give same results"""
```
- **Valida:** Batch encoding == loop de single encodes

##### `test_large_batch_performance(model)`
```python
def test_large_batch_performance(model):
    """Large batch (1000 docs) completes"""
```
- **Valida:** 1000 documentos se procesan sin crash
- **Duraci√≥n:** ~5-10 seg

---

#### **Categor√≠a 5: GPU Acceleration (1 test)**

##### `test_gpu_acceleration_functional(model)`
```python
def test_gpu_acceleration_functional(model):
    """GPU encoding faster than CPU"""
```
- **Valida:** GPU > CPU en velocidad
- **Speedup esperado:** >2x

---

#### **Categor√≠a 6: Error Handling (2 tests)**

##### `test_invalid_model_name_handling()`
```python
def test_invalid_model_name_handling():
    """Invalid model name raises error"""
```
- **Valida:** Exception con modelo inexistente
- **Edge case:** "nonexistent/fake-model-xyz"

##### `test_null_input_handling(model)`
```python
def test_null_input_handling(model):
    """Null/None inputs handled"""
```
- **Valida:** No crash con None en lista
- **Edge case:** [None, "text", None]

---

### Fixtures Disponibles

```python
@pytest.fixture(scope="module")
def model():
    """Shared model instance for all tests"""
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
```

**Ventaja:** Modelo se carga 1 sola vez, todos los tests lo reusan

---

## üöÄ Gu√≠a de Uso R√°pida

### Workflow Completo (RPVEA-A)

```bash
# PASO 1: PREPARE - Generar tests (ya hecho por @test-architect)
# ‚úÖ Tests ya existen

# PASO 2: VALIDATE - Ejecutar PRE-tests
python tests/pre/test_embedding_prereqs_20251002.py

# Si PRE-tests pasan:
# PASO 3: EXECUTE - Ejecutar benchmark
python benchmarks/scripts/embedding_benchmark.py | tee benchmarks/results/raw/embedding_output_20251002.txt

# PASO 4: ASSESS - Ejecutar POST-tests
python tests/post/test_embedding_success_20251002.py

# PASO 5: (Opcional) Integration tests
pytest tests/integration/test_embedding_pipeline_20251002.py -v
```

### Solo Validar Environment (R√°pido)

```bash
# Solo tests cr√≠ticos (< 30 seg)
pytest tests/pre/test_embedding_prereqs_20251002.py -k "cuda or gpu or packages" -v
```

### Debugging Tests Fallidos

```bash
# Ver output completo con prints
pytest tests/pre/test_embedding_prereqs_20251002.py -v -s

# Parar en primer fallo
pytest tests/pre/test_embedding_prereqs_20251002.py -x

# Ver traceback completo
pytest tests/pre/test_embedding_prereqs_20251002.py --tb=long
```

---

## üìä M√©tricas de Testing

### Coverage Summary

| Test Suite | Tests | Categor√≠as | Duraci√≥n Estimada |
|-------------|-------|------------|-------------------|
| PRE-tests | 17 | 5 (Environment, GPU, Models, Dependencies, FS) | 30-60 seg |
| POST-tests | 12 | 4 (Execution, Performance, Quality, Resources) | 15-20 min* |
| Integration | 16 | 6 (Loading, Encoding, Quality, Batch, GPU, Errors) | 2-5 min* |

**\*Duraci√≥n:** Incluye tiempo de ejecuci√≥n del benchmark/cargas de modelo

### Test Criticality Distribution

| Nivel | PRE | POST | Integration | Total |
|-------|-----|------|-------------|-------|
| üî¥ HIGH | 7 | 4 | 3 | 14 (31%) |
| ‚ö†Ô∏è MEDIUM | 6 | 5 | 2 | 13 (29%) |
| üü° LOW | 4 | 3 | 11 | 18 (40%) |

---

## üîß Troubleshooting Com√∫n

### PRE-test: test_model_multilingual_exists() falla con 401

**Problema:** HF API requiere autenticaci√≥n

**Soluciones:**
1. **Ignorar si modelo en cach√©:** Verificar con `ls ~/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2`
2. **Autenticar:** `huggingface-cli login`
3. **Modificar test:** Cambiar validaci√≥n API por carga local

### POST-test tarda mucho (>30 min)

**Problema:** Primera ejecuci√≥n descarga modelos

**Soluciones:**
1. **Pre-descargar modelos:**
   ```python
   from sentence_transformers import SentenceTransformer
   SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
   SentenceTransformer('dariolopez/bge-m3-es-legal-tmp-6')
   ```
2. **Skip POST-tests:** Ejecutar benchmark manualmente primero

### Integration tests fallan con "model not found"

**Problema:** Fixture no pudo cargar modelo

**Soluciones:**
1. Verificar internet conectado (primera descarga)
2. Limpiar cach√© corrupto: `rm -rf ~/.cache/huggingface/hub`
3. Re-instalar: `pip install --upgrade sentence-transformers`

---

## üìù Changelog

**2025-10-02:**
- Suite de tests generada por @test-architect
- 45 tests totales (17 PRE, 12 POST, 16 Integration)
- Cobertura completa de embedding benchmark pipeline
- Documentaci√≥n inicial de reference

**Futuras Expansiones:**
- [ ] Tests para LLM inference benchmarks
- [ ] Tests para dual-GPU configurations
- [ ] Tests para quantization comparisons
- [ ] Performance regression tests (comparar runs hist√≥ricos)

---

## üîó Referencias

**Archivos de Tests:**
- PRE: `C:\Users\Gamer\Dev\LLM-Local-Lab\tests\pre\test_embedding_prereqs_20251002.py`
- POST: `C:\Users\Gamer\Dev\LLM-Local-Lab\tests\post\test_embedding_success_20251002.py`
- Integration: `C:\Users\Gamer\Dev\LLM-Local-Lab\tests\integration\test_embedding_pipeline_20251002.py`

**Documentaci√≥n Relacionada:**
- Testing Strategy: `docs/testing/strategy_embedding_benchmark_20251002.md`
- RPVEA-A Methodology: `docs/workflows/rpvea-agent-integration.md`
- @test-architect Agent: `.claude/agents/test-architect.md`

**Mantenido por:** LLM-Local-Lab (RPVEA-A methodology)
